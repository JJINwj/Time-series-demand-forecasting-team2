{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP9MB5RLu792DRp3x+xb4t4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"N0wegR6AFG7-"},"outputs":[],"source":["########## Raw Data ë‹¤ìš´ ##########\n","\n","import requests\n","import pandas as pd\n","import time\n","import json\n","from datetime import datetime\n","\n","symbol = \"BTCUSDT\"\n","interval = \"1h\"\n","limit = 1000\n","\n","# 2017-08-17 00:00 UTC (Binance BTC ìµœì´ˆ ë°ì´í„° ì‹œì )\n","start_timestamp = int(pd.Timestamp(\"2017-08-17 00:00:00\", tz='UTC').timestamp() * 1000)\n","\n","checkpoint_file = \"checkpoint.json\"\n","output_file = \"btc_raw.csv\"\n","\n","# ------------------------------------------\n","# Load checkpoint\n","# ------------------------------------------\n","def load_checkpoint():\n","    try:\n","        with open(checkpoint_file, \"r\") as f:\n","            return json.load(f)\n","    except:\n","        return {\"last_time\": start_timestamp}\n","\n","# ------------------------------------------\n","# Save checkpoint\n","# ------------------------------------------\n","def save_checkpoint(ts):\n","    with open(checkpoint_file, \"w\") as f:\n","        json.dump({\"last_time\": ts}, f)\n","\n","# ------------------------------------------\n","# Request to Binance API (with retry logic)\n","# ------------------------------------------\n","def fetch_klines(start_time):\n","    url = \"https://api.binance.com/api/v3/klines\"\n","    params = {\n","        \"symbol\": symbol,\n","        \"interval\": interval,\n","        \"startTime\": start_time,\n","        \"limit\": limit\n","    }\n","\n","    while True:\n","        try:\n","            r = requests.get(url, params=params, timeout=10)\n","\n","            if r.status_code == 200:\n","                return r.json()\n","\n","            # Binance rate limit\n","            if r.status_code in [418, 429]:\n","                print(\"Rate limit hit â†’ waiting 60 seconds...\")\n","                time.sleep(60)\n","\n","            # Temporary block or legal restriction\n","            elif r.status_code in [403, 451]:\n","                print(f\"Server blocked ({r.status_code}) â†’ waiting 30 seconds...\")\n","                time.sleep(30)\n","\n","            else:\n","                print(f\"Unexpected status {r.status_code}, retrying in 15s...\")\n","                time.sleep(15)\n","\n","        except Exception as e:\n","            print(\"Connection error:\", e, \"retry in 10s...\")\n","            time.sleep(10)\n","\n","# ------------------------------------------\n","# Main loop\n","# ------------------------------------------\n","if __name__ == \"__main__\":\n","\n","    cp = load_checkpoint()\n","    current_start = cp[\"last_time\"]\n","\n","    print(\"Starting download from:\", pd.to_datetime(current_start, unit='ms'))\n","\n","    end_time = int(time.time() * 1000)\n","    all_data = []\n","\n","    while current_start < end_time:\n","        klines = fetch_klines(current_start)\n","\n","        if not klines:\n","            print(\"No more data returned.\")\n","            break\n","\n","        all_data.extend(klines)\n","        last_close_time = klines[-1][6]\n","\n","        # Save checkpoint\n","        save_checkpoint(last_close_time)\n","        current_start = last_close_time\n","\n","        print(\n","            f\"Fetched {len(klines)} rows, \"\n","            f\"next start = {pd.to_datetime(current_start, unit='ms')}\"\n","        )\n","\n","        time.sleep(0.4)  # Prevent rate limit\n","\n","    print(\"Download complete. Converting to DataFrame...\")\n","\n","# ------------------------------------------\n","# Convert result to DataFrame\n","# ------------------------------------------\n","cols = [\n","    \"OpenTime\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\",\n","    \"CloseTime\", \"QuoteAssetVolume\", \"NumberOfTrades\",\n","    \"TakerBuyBase\", \"TakerBuyQuote\", \"Ignore\"\n","]\n","\n","df = pd.DataFrame(all_data, columns=cols)\n","\n","# Convert timestamps\n","df[\"OpenTime\"] = pd.to_datetime(df[\"OpenTime\"], unit=\"ms\")\n","df[\"CloseTime\"] = pd.to_datetime(df[\"CloseTime\"], unit=\"ms\")\n","\n","# Remove duplicates\n","df = df.drop_duplicates(subset=\"OpenTime\")\n","\n","# Sort\n","df = df.sort_values(\"OpenTime\")\n","\n","# âš  ì—¬ê¸°ì„œ í•„ìš”í•œ 5ê°œ ì»¬ëŸ¼ë§Œ ì„ íƒ\n","df = df[[\"OpenTime\", \"Open\", \"High\", \"Low\", \"Close\"]]\n","\n","# Save CSV\n","df.to_csv(output_file, index=False)\n","print(\"Saved:\", output_file)"]},{"cell_type":"code","source":["########## ì „ì²˜ë¦¬ ##########\n","\n","import pandas as pd\n","import numpy as np\n","\n","# =========================================================\n","# 1. Raw ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n","# =========================================================\n","df = pd.read_csv(\"btc_raw.csv\")\n","\n","# =========================================================\n","# 2. ì‹œê°„ ì¸ë±ìŠ¤ ì •ë¦¬ + ê²°ì¸¡ 29ê°œ ìë™ ë³´ê°„\n","# =========================================================\n","\n","# Binance ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” \"OpenTime\" ì»¬ëŸ¼ì´ ë‚ ì§œ\n","df.rename(columns={\"OpenTime\": \"Date\"}, inplace=True)\n","\n","df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","df = df.sort_values(\"Date\").reset_index(drop=True)\n","\n","# 1ì‹œê°„ ë‹¨ìœ„ timestamp ê°•ì œ ìƒì„±\n","df = df.set_index(\"Date\").asfreq(\"1h\")\n","\n","# OHLCëŠ” ëª¨ë‘ ì§ì „ê°’ìœ¼ë¡œ ì±„ì›€ (forward fill)\n","df[\"Open\"] = df[\"Open\"].ffill()\n","df[\"High\"] = df[\"High\"].ffill()\n","df[\"Low\"]  = df[\"Low\"].ffill()\n","df[\"Close\"] = df[\"Close\"].ffill()\n","\n","# index ë³µêµ¬\n","df = df.reset_index()\n","\n","# =========================================================\n","# 3. Feature Engineering\n","# =========================================================\n","\n","# 3.1 ì£¼ê°€ ê¸°ë°˜ í”¼ì²˜\n","df[\"log_return\"] = np.log(df[\"Close\"] / df[\"Close\"].shift(1))\n","df[\"abs_return\"] = df[\"log_return\"].abs()\n","df[\"high_low_range\"] = (df[\"High\"] - df[\"Low\"]) / df[\"Open\"]\n","\n","df[\"return_5\"] = (df[\"Close\"] - df[\"Close\"].shift(4)) / df[\"Close\"].shift(4)\n","df[\"return_24\"] = (df[\"Close\"] - df[\"Close\"].shift(23)) / df[\"Close\"].shift(23)\n","\n","df[\"close_open\"] = (df[\"Close\"] - df[\"Open\"]) / df[\"Open\"]\n","\n","# 3.2 ë´‰ ê¸°ë°˜ í”¼ì²˜\n","df[\"real_body\"] = (df[\"Close\"] - df[\"Open\"]).abs()\n","df[\"upper_shadow\"] = df[\"High\"] - df[[\"Open\", \"Close\"]].max(axis=1)\n","df[\"lower_shadow\"] = df[[\"Open\", \"Close\"]].min(axis=1) - df[\"Low\"]\n","df[\"range\"] = df[\"High\"] - df[\"Low\"]\n","\n","epsilon = 1e-9\n","safe_range = df[\"range\"] + epsilon\n","\n","df[\"body_ratio\"]  = df[\"real_body\"]   / safe_range\n","df[\"upper_ratio\"] = df[\"upper_shadow\"] / safe_range\n","df[\"lower_ratio\"] = df[\"lower_shadow\"] / safe_range\n","\n","# 3.3 ë³€ë™ì„± ê¸°ë°˜ í”¼ì²˜\n","df[\"rolling_vol_24\"] = df[\"log_return\"].rolling(24).std()\n","df[\"rolling_vol_168\"] = df[\"log_return\"].rolling(168).std()\n","\n","df[\"parkinson_vol\"] = np.sqrt(\n","    (1 / (4 * np.log(2))) * (np.log(df[\"High\"] / df[\"Low\"]) ** 2)\n",")\n","\n","# 3.4 ì¶”ì„¸ ê¸°ë°˜ í”¼ì²˜\n","df[\"SMA_5\"] = df[\"Close\"].rolling(5).mean()\n","df[\"SMA_10\"] = df[\"Close\"].rolling(10).mean()\n","df[\"SMA_20\"] = df[\"Close\"].rolling(20).mean()\n","\n","df[\"EMA_12\"] = df[\"Close\"].ewm(span=12, adjust=False).mean()\n","df[\"EMA_26\"] = df[\"Close\"].ewm(span=26, adjust=False).mean()\n","\n","df[\"MACD\"] = df[\"EMA_12\"] - df[\"EMA_26\"]\n","\n","# 3.5 ëª¨ë©˜í…€ ê¸°ë°˜ í”¼ì²˜ (RSI)\n","delta = df[\"Close\"].diff()\n","gain = delta.clip(lower=0)\n","loss = -delta.clip(upper=0)\n","\n","avg_gain = gain.rolling(14).mean()\n","avg_loss = loss.rolling(14).mean()\n","\n","RS = avg_gain / avg_loss\n","df[\"RSI_14\"] = 100 - (100 / (1 + RS))\n","\n","# 3.6 ê¸°ìˆ ì  ë³€ë™ì„± í”¼ì²˜\n","df[\"TR\"] = np.maximum.reduce([\n","    df[\"High\"] - df[\"Low\"],\n","    (df[\"High\"] - df[\"Close\"].shift(1)).abs(),\n","    (df[\"Low\"] - df[\"Close\"].shift(1)).abs()\n","])\n","\n","df[\"ATR_14\"] = df[\"TR\"].rolling(14).mean()\n","\n","# 3.7 ê³ ì°¨ëª¨ë©˜íŠ¸ í”¼ì²˜\n","df[\"skewness_24\"] = df[\"log_return\"].rolling(24).skew()\n","df[\"kurtosis_24\"] = df[\"log_return\"].rolling(24).kurt()\n","\n","# =========================================================\n","# 4. Up/Down ë¼ë²¨ ìƒì„±\n","# =========================================================\n","df[\"target\"] = (df[\"Close\"].shift(-1) > df[\"Close\"]).astype(int)\n","\n","df = df.iloc[:-1]  # ë§ˆì§€ë§‰ target ì—†ìŒ\n","\n","# =========================================================\n","# 5. ê²°ì¸¡ì¹˜ ì œê±° ë° ê¸°ê°„ ì œí•œ\n","# =========================================================\n","df = df.dropna()\n","df = df[df[\"Date\"] >= \"2020-01-01\"]\n","\n","# =========================================================\n","# 6. ì €ì¥\n","# =========================================================\n","df.to_csv(\"btc_processed.csv\", index=False)\n","\n","print(\"btc_processed.csv ìƒì„± ì™„ë£Œ!\")\n"],"metadata":{"id":"gOCu7qIHFaiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########## EDA ##########\n","\n","# ============================================================\n","# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n","# ============================================================\n","!pip install ydata-profiling\n","!pip install seaborn\n","!pip install xgboost\n","\n","# ============================================================\n","# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n","# ============================================================\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from ydata_profiling import ProfileReport\n","\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import train_test_split\n","\n","plt.style.use(\"seaborn-v0_8\")\n","sns.set_palette(\"deep\")\n","\n","# ============================================================\n","# 2. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬\n","# ============================================================\n","df = pd.read_csv(\"btc_processed.csv\")\n","\n","# Date ì»¬ëŸ¼ datetime ë³€í™˜\n","if \"Date\" in df.columns:\n","    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","\n","print(\"\\n===== ë°ì´í„° ê¸°ë³¸ ì •ë³´ =====\")\n","print(df.info())\n","print(df.head())\n","\n","# ============================================================\n","# 3. Target ë¹„ìœ¨\n","# ============================================================\n","# Target ë¹„ìœ¨ ì‹œê°í™”\n","plt.figure(figsize=(4,4))\n","df[\"target\"].value_counts(normalize=True).plot(kind='bar')\n","plt.title(\"Target Class Distribution\")\n","plt.xticks([0,1], [\"Down(0)\", \"Up(1)\"], rotation=0)\n","plt.show()\n","\n","# ============================================================\n","# 4. Profiling Report ìƒì„±\n","# ============================================================\n","profile = ProfileReport(df, title=\"BTC Profiling Report\")\n","profile.to_file(\"btc_profiling_report.html\")\n","\n","# ============================================================\n","# 5. ë”¥ëŸ¬ë‹ìš© EDA (OHLC ì¤‘ì‹¬)\n","# ============================================================\n","\n","# OHLC ì‹œê³„ì—´\n","plt.figure(figsize=(14,4))\n","plt.plot(df[\"Date\"], df[\"Close\"], linewidth=0.7)\n","plt.title(\"BTC Close Price Over Time\")\n","plt.xlabel(\"Date\"); plt.ylabel(\"Close Price\")\n","plt.show()\n","\n","# ë³€ë™ì„±\n","df[\"volatility\"] = df[\"High\"] - df[\"Low\"]\n","plt.figure(figsize=(14,4))\n","plt.plot(df[\"Date\"], df[\"volatility\"], color='orange')\n","plt.title(\"Volatility (High - Low) Over Time\")\n","plt.xlabel(\"Date\"); plt.ylabel(\"Volatility\")\n","plt.show()\n","\n","# ìˆ˜ìµë¥ (return)\n","df[\"return\"] = df[\"Close\"].pct_change()\n","plt.figure(figsize=(6,4))\n","sns.histplot(df[\"return\"].dropna(), bins=100, kde=True)\n","plt.title(\"Return Distribution\")\n","plt.show()\n","\n","# Up/Downë³„ ìˆ˜ìµë¥  ë¶„í¬\n","plt.figure(figsize=(6,4))\n","sns.boxplot(x=df[\"target\"], y=df[\"return\"])\n","plt.title(\"Return by Target Class\")\n","plt.xticks([0,1], [\"Down\",\"Up\"])\n","plt.show()\n","\n","# ============================================================\n","# 6. ë¨¸ì‹ ëŸ¬ë‹ìš© Feature EDA (OHLC, Date ì œì™¸)\n","# ============================================================\n","drop_cols = [\"Open\",\"High\",\"Low\",\"Close\",\"Date\",\"target\",\"return\",\"volatility\"]\n","ml_features = df.drop(columns=[c for c in drop_cols if c in df.columns])\n","\n","# ìƒê´€ ë¶„ì„\n","plt.figure(figsize=(12,8))\n","sns.heatmap(ml_features.corr(), cmap=\"coolwarm\", center=0)\n","plt.title(\"Correlation Heatmap (ML Features)\")\n","plt.show()\n","\n","# Targetê³¼ ê° feature ê´€ê³„ í™•ì¸\n","for col in ml_features.columns:\n","    plt.figure(figsize=(6,4))\n","    sns.boxplot(x=df[\"target\"], y=df[col])\n","    plt.title(f\"{col} vs Target\")\n","    plt.xticks([0,1], [\"Down\",\"Up\"])\n","    plt.show()\n","\n","# ============================================================\n","# 7. Feature Importance (Baseline XGBoost)\n","# ============================================================\n","X = ml_features\n","y = df[\"target\"]\n","\n","# ì‹œê³„ì—´ íŠ¹ì„± ìœ ì§€ â†’ shuffle=False\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, shuffle=False\n",")\n","\n","xgb = XGBClassifier(\n","    max_depth=4,\n","    learning_rate=0.1,\n","    n_estimators=200,\n","    eval_metric=\"logloss\"\n",")\n","\n","xgb.fit(X_train, y_train)\n","\n","importance = pd.Series(\n","    xgb.feature_importances_,\n","    index=X.columns\n",").sort_values(ascending=False)\n","\n","plt.figure(figsize=(12,4))\n","importance.plot(kind=\"bar\")\n","plt.title(\"XGBoost Feature Importance\")\n","plt.show()\n","\n","# ============================================================\n","# 8. Train vs Test ë¶„í¬ ë¹„êµ (ì‹œê³„ì—´ drift ì—¬ë¶€)\n","# ============================================================\n","train = df.iloc[:-5000]\n","train = df.iloc[:-5000]\n","test = df.iloc[-5000:]\n","\n","plt.figure(figsize=(10,4))\n","sns.kdeplot(train[\"return\"].dropna(), label=\"Train\")\n","sns.kdeplot(test[\"return\"].dropna(), label=\"Test\")\n","plt.title(\"Train vs Test Return Distribution\")\n","plt.legend()\n","plt.show()\n","\n","print(\"\\nğŸ‰ ì „ì²´ EDA ì™„ë£Œ! ìƒì„±ëœ ë¦¬í¬íŠ¸ íŒŒì¼:\")\n","print(\"- btc_profiling_report.html (ydata profiling)\")\n"],"metadata":{"id":"TggQYe3eFlqA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########## ë”¥ëŸ¬ë‹ ##########\n","\n","# ----------------------------------------------------------------------\n","# 0. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (Colab í™˜ê²½ ê¸°ì¤€)\n","# ----------------------------------------------------------------------\n","\n","!pip install darts[u] autogluon.tabular shap captum\n","\n","# ----------------------------------------------------------------------\n","# 1. í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ë¡œë”©\n","# ----------------------------------------------------------------------\n","\n","# ----------------------------------------------------------------------\n","# 1-1. ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\n","# ----------------------------------------------------------------------\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.metrics import (\n","    mean_squared_error,\n","    mean_absolute_error,\n","    accuracy_score,\n","    f1_score,\n","    confusion_matrix,\n","    roc_curve,\n","    auc\n",")\n","\n","import torch\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# DARTS\n","from darts import TimeSeries\n","from darts.dataprocessing.transformers import Scaler\n","from darts.models import BlockRNNModel, TCNModel, TransformerModel\n","from darts.metrics import mape\n","\n","# AutoGluon\n","from autogluon.tabular import TabularPredictor\n","\n","# SHAP\n","import shap\n","\n","# ----------------------------------------------------------------------\n","# 1-2. Raw ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n","# ----------------------------------------------------------------------\n","\n","DATA_PATH = \"btc_processed.csv\"\n","df = pd.read_csv(DATA_PATH)\n","\n","# Date -> datetime\n","df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n","df = df.sort_values(\"Date\").reset_index(drop=True)\n","\n","print(\"ë°ì´í„° ì»¬ëŸ¼:\", df.columns.tolist())\n","print(df.head())\n","\n","# ----------------------------------------------------------------------\n","# 2. DARTS ê¸°ë°˜ ë”¥ëŸ¬ë‹ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸\n","# ----------------------------------------------------------------------\n","\n","# ----------------------------------------------------------------------\n","# 2-1. TimeSeries ìƒì„± (Target=Close / Covariates=OHLC)\n","# ----------------------------------------------------------------------\n","\n","# target: Close (univariate)\n","series_y = TimeSeries.from_dataframe(\n","    df,\n","    time_col=\"Date\",\n","    value_cols=[\"Close\"],\n","    freq=None\n",")\n","\n","# features: OHLC (multivariate)\n","series_X = TimeSeries.from_dataframe(\n","    df,\n","    time_col=\"Date\",\n","    value_cols=[\"Open\", \"High\", \"Low\", \"Close\"],\n","    freq=None\n",")\n","\n","print(\"series_y:\", series_y)\n","print(\"series_X:\", series_X)\n","\n","# ----------------------------------------------------------------------\n","# 2-2. ì‹œê³„ì—´ ìŠ¤ì¼€ì¼ë§ (ì…ë ¥Â·ì¶œë ¥ ê°œë³„ ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©)\n","# ----------------------------------------------------------------------\n","\n","scaler_y = Scaler()\n","scaler_X = Scaler()\n","\n","series_y_scaled = scaler_y.fit_transform(series_y)\n","series_X_scaled = scaler_X.fit_transform(series_X)\n","\n","# ----------------------------------------------------------------------\n","# 2-3. ì‹œê³„ì—´ ë°ì´í„° ë¶„í•  (Train/Val/Test; 85%/15%)\n","# ----------------------------------------------------------------------\n","\n","N = len(series_y_scaled)\n","\n","N_train_val = int(N * 0.85)\n","N_train     = int(N_train_val * 0.8)\n","N_val       = N_train_val - N_train\n","N_test      = N - N_train_val\n","\n","print(\"ì´ ê¸¸ì´ N:\", N)\n","print(\"train:\", N_train, \"val:\", N_val, \"test:\", N_test)\n","\n","# y split\n","train_y, valtest_y = series_y_scaled.split_before(N_train)\n","val_y,   test_y    = valtest_y.split_before(N_val)\n","\n","# X split\n","train_X, valtest_X = series_X_scaled.split_before(N_train)\n","val_X,   test_X    = valtest_X.split_before(N_val)\n","\n","print(\"len(train_y), len(val_y), len(test_y):\",\n","      len(train_y), len(val_y), len(test_y))\n","\n","# ----------------------------------------------------------------------\n","# 2-4. AutoDL í›„ë³´ ëª¨ë¸ ì •ì˜ (LSTM / GRU / TCN / Transformer)\n","# ----------------------------------------------------------------------\n","\n","input_chunk_length = 168\n","output_chunk_length = 1\n","\n","common_torch_kwargs = dict(\n","    input_chunk_length=input_chunk_length,\n","    output_chunk_length=output_chunk_length,\n","    n_epochs=3,\n","    batch_size=32,\n","    random_state=42,\n","    optimizer_kwargs={\"lr\": 1e-3},\n","    force_reset=True,\n","    save_checkpoints=True,\n",")\n","\n","models = []\n","\n","# 1) LSTM\n","models.append((\n","    \"LSTM\",\n","    BlockRNNModel(\n","        model=\"LSTM\",\n","        **common_torch_kwargs,\n","        model_name=\"LSTM_btc_close\"\n","    )\n","))\n","\n","# 2) GRU\n","models.append((\n","    \"GRU\",\n","    BlockRNNModel(\n","        model=\"GRU\",\n","        **common_torch_kwargs,\n","        model_name=\"GRU_btc_close\"\n","    )\n","))\n","\n","# 3) TCN\n","models.append((\n","    \"TCN\",\n","    TCNModel(\n","        **common_torch_kwargs,\n","        model_name=\"TCN_btc_close\"\n","    )\n","))\n","\n","# 4) Transformer\n","models.append((\n","    \"Transformer\",\n","    TransformerModel(\n","        **common_torch_kwargs,\n","        model_name=\"Transformer_btc_close\"\n","    )\n","))\n","\n","# ----------------------------------------------------------------------\n","# 2-5. ëª¨ë¸ í•™ìŠµ ë° Validation MAPE ê¸°ë°˜ Best Model ì„ íƒ\n","# ----------------------------------------------------------------------\n","\n","best_mape = np.inf\n","best_model = None\n","best_name  = None\n","val_results = {}\n","\n","for name, model in models:\n","    print(\"\\n==============================\")\n","    print(f\"â–¶ Training model: {name}\")\n","    print(\"==============================\")\n","\n","    model.fit(\n","        series=train_y,\n","        past_covariates=train_X,\n","        val_series=val_y,\n","        val_past_covariates=val_X,\n","        verbose=True,\n","    )\n","\n","    # Validation ì˜ˆì¸¡\n","    pred_val_scaled = model.predict(\n","        n=len(val_y),\n","        past_covariates=series_X_scaled\n","    )\n","\n","    # ì—­ìŠ¤ì¼€ì¼ë§\n","    val_pred = scaler_y.inverse_transform(pred_val_scaled)\n","    val_true = scaler_y.inverse_transform(val_y)\n","\n","    mape_val = mape(val_true, val_pred)\n","    val_results[name] = mape_val\n","\n","    print(f\"ğŸ”¹ {name} Val MAPE: {mape_val:.4f}\")\n","\n","    if mape_val < best_mape:\n","        best_mape = mape_val\n","        best_model = model\n","        best_name  = name\n","\n","print(\"\\nâœ… Best model on Validation (MAPE ê¸°ì¤€)\")\n","print(\"best_name :\", best_name)\n","print(\"best_mape :\", best_mape)\n","\n","# ----------------------------------------------------------------------\n","# 2-6. Test Forecast ìˆ˜í–‰ ë° RMSE/MAE ì„±ëŠ¥ í‰ê°€\n","# ----------------------------------------------------------------------\n","\n","horizon = len(test_y)\n","\n","pred_test_scaled = best_model.predict(\n","    n=len(test_y),\n","    past_covariates=series_X_scaled,\n",")\n","\n","# ì—­ìŠ¤ì¼€ì¼ë§\n","pred_test = scaler_y.inverse_transform(pred_test_scaled)\n","test_true = scaler_y.inverse_transform(test_y)\n","\n","y_true = test_true.values().flatten()\n","y_pred = pred_test.values().flatten()\n","\n","rmse_value = np.sqrt(mean_squared_error(y_true, y_pred))\n","mae_value  = mean_absolute_error(y_true, y_pred)\n","\n","print(\"\\nğŸ“Œ DARTS DL íšŒê·€ ì„±ëŠ¥\")\n","print(f\"RMSE: {rmse_value:.4f}\")\n","print(f\"MAE : {mae_value:.4f}\")\n","\n","# ----------------------------------------------------------------------\n","# 2-7. Close ì˜ˆì¸¡ ê¸°ë°˜ Up/Down ë°©í–¥ì„± ë¶„ë¥˜ ë° ì •í™•ë„ í‰ê°€\n","# ----------------------------------------------------------------------\n","\n","# true_diff / pred_diff ì •ì˜\n","true_diff = np.diff(y_true)\n","pred_diff = np.diff(y_pred)\n","\n","y_true_cls = (true_diff > 0).astype(int)\n","y_pred_cls = (pred_diff > 0).astype(int)\n","\n","acc_cls = accuracy_score(y_true_cls, y_pred_cls)\n","f1_cls  = f1_score(y_true_cls, y_pred_cls)\n","cm      = confusion_matrix(y_true_cls, y_pred_cls)\n","\n","print(\"\\nğŸ“Œ DL Direction Classification (Up/Down)\")\n","print(f\"Accuracy: {acc_cls:.4f}\")\n","print(f\"F1      : {f1_cls:.4f}\")\n","\n","# Confusion Matrix\n","plt.figure(figsize=(5, 4))\n","sns.heatmap(\n","    cm,\n","    annot=True,\n","    fmt=\"d\",\n","    cmap=\"Blues\",\n","    linecolor=None,\n","    linewidths=0,\n","    xticklabels=[\"Pred Down\", \"Pred Up\"],\n","    yticklabels=[\"True Down\", \"True Up\"]\n",")\n","\n","plt.grid(False)\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix (Up/Down Classification)\")\n","plt.show()\n","\n","# ROC curve (scoreë¡œ pred_diff ì‚¬ìš©)\n","fpr, tpr, thresholds = roc_curve(y_true_cls, pred_diff)\n","roc_auc = auc(fpr, tpr)\n","\n","print(f\"ROC AUC: {roc_auc:.4f}\")\n","\n","# ROC plot\n","plt.figure(figsize=(5, 5))\n","plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n","plt.plot([0, 1], [0, 1], linestyle=\"--\")\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"DL Direction Classification ROC\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# ----------------------------------------------------------------------\n","# 2-8. Surrogate SHAP ê¸°ë°˜ DARTS Best Model í•´ì„(XAI)\n","# ----------------------------------------------------------------------\n","\n","import shap\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import r2_score\n","\n","# 1. DARTS ì˜ˆì¸¡ ê²°ê³¼ ì¤€ë¹„\n","print(\"y_pred shape:\", y_pred.shape)\n","\n","# 2. TimeSeries â†’ numpy ë³€í™˜\n","vals_X = test_X.all_values(copy=False)    # (N_test, F, 1)\n","vals_X = np.squeeze(vals_X, axis=2)       # (N_test, F)\n","N_test, F = vals_X.shape\n","\n","input_len = best_model.input_chunk_length\n","print(f\"Input length = {input_len}, Features = {F}\")\n","\n","# 3. Sliding window ìƒì„±\n","def create_windows(arr, win):\n","    X = []\n","    for i in range(win, len(arr)):\n","        X.append(arr[i-win:i, :])     # (win, F)\n","    return np.array(X)\n","\n","X_windows = create_windows(vals_X, input_len)   # (N_samples, T, F)\n","y_windows = y_pred[input_len:]                  # ëŒ€ì‘ë˜ëŠ” ì˜ˆì¸¡ê°’\n","\n","print(\"X_windows shape:\", X_windows.shape)\n","print(\"y_windows shape:\", y_windows.shape)\n","\n","# 4. SHAP ì…ë ¥ì„ ìœ„í•´ í‰íƒ„í™” (Samples, T*F)\n","X_flat = X_windows.reshape(X_windows.shape[0], -1)\n","N_samples, N_features_flat = X_flat.shape\n","print(\"Flattened input shape:\", X_flat.shape)\n","\n","# 5. Surrogate Model (RandomForest) í•™ìŠµ\n","rf = RandomForestRegressor(\n","    n_estimators=400,\n","    max_depth=None,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","rf.fit(X_flat, y_windows)\n","y_hat = rf.predict(X_flat)\n","\n","print(\"\\nğŸ“Œ Surrogate RÂ² (RF vs DARTS predictions):\", r2_score(y_windows, y_hat))\n","print(\"ğŸ‘‰ Surrogateê°€ ë†’ì„ìˆ˜ë¡ SHAP ì„¤ëª…ë ¥ì´ ì¢‹ìŒ (0.9 ì´ìƒ ê¶Œì¥)\")\n","\n","# 6. SHAP TreeExplainer\n","explainer = shap.TreeExplainer(rf)\n","\n","# ë¶„ì„í•  ìƒ˜í”Œ ìˆ˜ (ë„ˆë¬´ í¬ë©´ ëŠë¦¬ë¯€ë¡œ 200ê°œ ê¶Œì¥)\n","n_explain = min(200, N_samples)\n","X_explain = X_flat[-n_explain:]\n","\n","shap_values = explainer.shap_values(X_explain)\n","shap_values = np.array(shap_values)   # (N_explain, T*F)\n","\n","print(\"SHAP values shape:\", shap_values.shape)\n","\n","# 7. Feature ì´ë¦„ ìƒì„± (ì˜ˆ: Close_lag0, High_lag23 ...)\n","columns = test_X.columns  # ['Open', 'High', 'Low', 'Close']\n","feature_names = []\n","\n","for t in range(input_len):\n","    lag = input_len - 1 - t\n","    for col in columns:\n","        feature_names.append(f\"{col}_lag{lag}\")\n","\n","assert len(feature_names) == N_features_flat\n","\n","# 8. Global SHAP Bar Plot (ìƒìœ„ ì¤‘ìš” feature)\n","plt.figure(figsize=(10, 6))\n","plt.title(\"SHAP Feature Importance (Surrogate for DARTS)\")\n","shap.summary_plot(shap_values, X_explain, feature_names=feature_names, plot_type=\"bar\")\n","plt.show()\n","\n","# 9. SHAP Summary Plot (color scatter)\n","plt.figure(figsize=(10, 6))\n","shap.summary_plot(shap_values, X_explain, feature_names=feature_names)\n","plt.show()\n","\n","# 10. Time Ã— Feature Heatmap (ê°œë³„ ìƒ˜í”Œ í•´ì„)\n","idx = -1   # ë§ˆì§€ë§‰ ìƒ˜í”Œ ì„¤ëª…\n","sv = shap_values[idx].reshape(input_len, F).T   # (F, T)\n","\n","plt.figure(figsize=(12, 3))\n","plt.imshow(sv, aspect=\"auto\", cmap=\"bwr\")\n","plt.colorbar(label=\"SHAP value\")\n","plt.yticks(range(F), columns)\n","plt.xlabel(f\"Time step (past {input_len} hours)\")\n","plt.title(\"SHAP Heatmap: Timestep Ã— Feature Importance\")\n","plt.show()\n","\n","# 11. Global OHLC Feature Importance (ì „ì²´ lag í‰ê· )\n","mean_abs_shap = np.mean(np.abs(shap_values), axis=0)     # (T*F,)\n","mean_abs_shap_mat = mean_abs_shap.reshape(input_len, F)  # (T, F)\n","feat_importance = mean_abs_shap_mat.mean(axis=0)         # (F,)\n","\n","plt.figure(figsize=(5,3))\n","plt.bar(columns, feat_importance)\n","plt.ylabel(\"Mean |SHAP|\")\n","plt.title(\"Global Feature Importance (OHLC aggregated)\")\n","plt.show()\n","\n","print(\"\\nğŸ“Œ OHLC aggregated importance:\")\n","for col, val in zip(columns, feat_importance):\n","    print(f\"{col}: {val:.6f}\")\n"],"metadata":{"id":"xcz9I838FemR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########## ë¨¸ì‹ ëŸ¬ë‹ ##########\n","\n","# ----------------------------------------------------------------------\n","# 3. AutoGluon ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ë°©í–¥ì„± ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸\n","# ----------------------------------------------------------------------\n","\n","# ----------------------------------------------------------------------\n","# 3-1. ML Feature Engineering (engineered features â†’ lag ë³€í™˜)\n","# ----------------------------------------------------------------------\n","df_ml = df.copy()\n","\n","drop_cols = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\"]\n","drop_cols = [c for c in drop_cols if c in df_ml.columns]\n","\n","feature_cols = [c for c in df_ml.columns if c not in (drop_cols + [\"target\"])]\n","\n","data_ml = df_ml[feature_cols + [\"target\"]].copy()\n","\n","print(\"ì‚¬ìš© feature ìˆ˜:\", len(feature_cols))\n","print(\"feature_cols:\", feature_cols)\n","\n","# ----------------------------------------------------------------------\n","# 3-2. ì‹œê³„ì—´ Split (Train/Val/Test; 85%/15%)\n","# ----------------------------------------------------------------------\n","\n","N_ml = len(data_ml)\n","N_ml_train_val = int(N_ml * 0.85)\n","\n","train_ml = data_ml.iloc[:N_ml_train_val].reset_index(drop=True)\n","test_ml  = data_ml.iloc[N_ml_train_val:].reset_index(drop=True)\n","\n","print(\"ML train size:\", len(train_ml), \"test size:\", len(test_ml))\n","\n","# ----------------------------------------------------------------------\n","# 3-3. AutoGluon TabularPredictor í•™ìŠµ\n","# ----------------------------------------------------------------------\n","\n","label = \"target\"\n","\n","predictor = TabularPredictor(\n","    label=label,\n","    problem_type=\"binary\",\n","    eval_metric=\"f1\"\n",").fit(\n","    train_data=train_ml,\n","    presets=\"good_quality\",\n","    time_limit=600,    # 10ë¶„\n",")\n","\n","leaderboard = predictor.leaderboard(silent=True)\n","print(\"\\nAutoGluon Leaderboard:\")\n","print(leaderboard)\n","\n","# ----------------------------------------------------------------------\n","# 3-4. AutoGluon ë°©í–¥ì„± ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€ (Accuracy / F1 / CM / ROC)\n","# ----------------------------------------------------------------------\n","\n","X_test_ml = test_ml[feature_cols]\n","y_test_ml = test_ml[label]\n","\n","pred_ml      = predictor.predict(test_ml)\n","proba_ml_df  = predictor.predict_proba(test_ml)\n","\n","# ì–‘ì„± í´ë˜ìŠ¤(1)ì˜ í™•ë¥ \n","if isinstance(proba_ml_df, pd.DataFrame):\n","    if 1 in proba_ml_df.columns:\n","        prob_ml = proba_ml_df[1].values\n","    else:\n","        prob_ml = proba_ml_df.iloc[:, -1].values\n","else:\n","    prob_ml = proba_ml_df\n","\n","acc_ml = accuracy_score(y_test_ml, pred_ml)\n","f1_ml  = f1_score(y_test_ml, pred_ml)\n","cm_ml  = confusion_matrix(y_test_ml, pred_ml)\n","\n","fpr_ml, tpr_ml, thr_ml = roc_curve(y_test_ml, prob_ml)\n","auc_ml = auc(fpr_ml, tpr_ml)\n","\n","print(\"\\nğŸ“Œ AutoGluon ML Direction Classification\")\n","print(f\"Accuracy: {acc_ml:.4f}\")\n","print(f\"F1      : {f1_ml:.4f}\")\n","\n","# Confusion Matrix\n","plt.figure(figsize=(5, 4))\n","sns.heatmap(\n","    cm_ml,\n","    annot=True,\n","    fmt=\"d\",\n","    cmap=\"Blues\",\n","    linewidths=0,\n","    linecolor=\"white\",\n","    xticklabels=[\"Pred Down\", \"Pred Up\"],\n","    yticklabels=[\"True Down\", \"True Up\"]\n",")\n","plt.grid(False)\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix (AutoGluon ML)\")\n","plt.show()\n","\n","print(f\"ROC AUC : {auc_ml:.4f}\")\n","plt.figure(figsize=(5, 5))\n","plt.plot(fpr_ml, tpr_ml, label=f\"AUC = {auc_ml:.3f}\")\n","plt.plot([0, 1], [0, 1], linestyle=\"--\")\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate\")\n","plt.title(\"AutoGluon ML ROC\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# ----------------------------------------------------------------------\n","# 3-5. SHAP ê¸°ë°˜ ML ëª¨ë¸ ì„¤ëª… (Summary / Bar / Dependence)\n","# ----------------------------------------------------------------------\n","\n","X_train_ml = train_ml[feature_cols]\n","X_test_ml  = test_ml[feature_cols]\n","\n","# AutoGluon ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ SHAPìš©ìœ¼ë¡œ ë˜í•‘\n","def ag_predict_proba(X):\n","    X_df = pd.DataFrame(X, columns=feature_cols)\n","    proba = predictor.predict_proba(X_df)\n","    if isinstance(proba, pd.DataFrame):\n","        if 1 in proba.columns:\n","            return proba[1].values\n","        else:\n","            return proba.iloc[:, -1].values\n","    else:\n","        return proba\n","\n","# background / explain sample ì„ íƒ\n","background = shap.sample(X_train_ml, 200, random_state=0)\n","explainer  = shap.KernelExplainer(ag_predict_proba, background)\n","\n","X_explain = shap.sample(X_test_ml, 200, random_state=1)\n","\n","# nsamplesëŠ” ì†ë„ì— ë”°ë¼ ì¡°ì ˆ\n","shap_values = explainer.shap_values(X_explain, nsamples=100)\n","\n","# Summary plot (feature ì¤‘ìš”ë„ + ë°©í–¥ì„±)\n","shap.summary_plot(\n","    shap_values,\n","    X_explain,\n","    feature_names=feature_cols,\n","    show=True\n",")\n","plt.rcParams['axes.grid'] = False\n","# Bar plot (mean |SHAP| ê¸°ì¤€)\n","shap.summary_plot(\n","    shap_values,\n","    X_explain,\n","    feature_names=feature_cols,\n","    plot_type=\"bar\",\n","    show=True\n",")\n","\n","# Dependence plot (ì˜ˆ: ìƒìœ„ important feature í•˜ë‚˜ ê³¨ë¼ì„œ)\n","top_feature = feature_cols[0]  # ì‹¤ì œë¡  summary ê²°ê³¼ ë³´ê³  ë³€ê²½ ê¶Œì¥\n","shap.dependence_plot(\n","    top_feature,\n","    shap_values,\n","    X_explain,\n","    feature_names=feature_cols,\n","    show=True\n",")\n"],"metadata":{"id":"BiML-GmMF0fi"},"execution_count":null,"outputs":[]}]}